<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Blog</title><meta name="description" content="A Blog Powered By Hexo"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/theme.css"><link rel="search" type="application/opensearchdescription+xml" href="/atom.xml" title="Blog"></head><body><div class="wrap"><header><h1 class="branding"><a href="/" title="Blog">Blog</a></h1><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link active" href="/" target="_self">HOME</a></li><li class="nav-list-item"><a class="nav-list-link" href="https://github.com/mathijshenquet" target="_blank">GITHUB</a></li><li class="nav-list-item"><a class="nav-list-link" href="/atom.xml" target="_self">RSS</a></li></ul></header><main class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h1 class="post-title"><a class="post-title-link" href="/2017/08/15/random/">Random</a></h1><div class="post-info"><a></a>2017-08-15</div><div class="post-content"><p>Intuitively things are only interesting if there is some informational content to them. This can be made formal by using the <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" target="_blank" rel="external">Kolomogrov complexity</a> measure which expresses the size of this information content or complexity. We call the ratio between a sentence’s length and its complexity is its compression. So a sentence that compresses well has little complexity.</p>
<p>Unfortunately, using complexity as a measure of value introduces pathologies. The sentences that are least compressible are random sentences (as there are no patterns to exploit). There are information theoretic ways to fix this, nicely <a href="http://www.scottaaronson.com/blog/?p=762" target="_blank" rel="external">explored by Scott Aaronson</a>. I want to take a different approach and analyse the nature of these random strings.</p>
<p>We essentially deem the random strings to be of no value because they give is no predictive strength over our environment. We say that the information in the string is not (causally) entangled with reality. But is this really the case? Physics teaches us that random strings don’t really exists, as von Neumann said:</p>
<p>   Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin. For, as has been pointed out several times,<br>   there is no such thing as a random number</p>
<p>The random strings in our computer are usually from something physicists call thermal energy. Thermal energy is the micro level jiggling of particles that do not contribute to the macro level state.</p>
<p>Thermal physics teaches us that this thermal energy is not random after all, but degrees of freedom (= information) that has become inaccessible to us. To make this precise: All the thermal energy in a system is in principle enough to allow us to derive all past and future states. But the information is not localized in any structured way so we would need to know all the particles to make sense of it. The inaccessible information is smeared out through all the particles. This is in contrast with accessible information which is localized, like a bit in computer memory.</p>
<p>From this it is clear that randomness is but a matter of perspective, a sufficiently omnipotent being sees no randomness at all, and nature’s weather appeared random to earlier humans. </p>
<p>Accepting that computer randomness is obtained from terminal noise we see that random strings <em>do</em> posses a lot of useful information, but it too tangled up to be of any use. This suggests that we should explore the ways in which information is entangled, or to be more specific gives us predictive power, to assess its value.</p>
</div><a class="read-more" href="/2017/08/15/random/">&hellip; more</a></article></li><li class="post-list-item"><article class="post-block"><h1 class="post-title"><a class="post-title-link" href="/2017/08/15/rules/">Rules</a></h1><div class="post-info"><a></a>2017-08-15</div><div class="post-content"><p>Why do we live in a universe where everything happens by essentially[^1] deterministic rules? This is a philosophical question I wont get into but I would like to give a mathematical perspective.</p>
<p>Suppose we want to simulate a toy universe. By formulating a finite set of rules we ensure that the universe’s step function is computable. Indeed without it we would need an infinite amount of information, either upfront or as-we-go, to serve as ‘inspiration’ for the next time steps. There is no way around this, for if the infinite amount of data is really generated by some rule, we can simply incorporate that rule into our set of rules which will then be finite again.</p>
<p>[^1] Ignoring the probabilistic nature of quantum physics for the moment.</p>
</div><a class="read-more" href="/2017/08/15/rules/">&hellip; more</a></article></li><li class="post-list-item"><article class="post-block"><h1 class="post-title"><a class="post-title-link" href="/2017/08/15/compression/">Compression</a></h1><div class="post-info"><a></a>2017-08-15</div><div class="post-content"><p>Any definition of conscience, and therefore intelligence, should somehow incorporate the amount of compression that the agent applies to its senses. In a way compression is the same as understanding in the following way. Suppose we have a sequence of data points, for example the sequence of even numbers. A priori this contains an infinite amount of information, which is impossible to store. But by employing patterns in the data, we can make a finite representation: informally “start at 0 and keep on adding +2”. </p>
<p>Depending on the expressive strength of the agents internal language we can build representations of more complex data[^1]. Depending on the reasoning capacity the agent can then find and exploit more and more patterns in the data allowing for efficient representation.</p>
<p>In practice an agent might be supplied some initial segment of a sequence, i.e. the observations of the first 100 even numbers. Using some form of (probabilistic[^2]) reasoning the agent builds an efficient representation as the informal sentence above. But not only is the data compressed by this, the agent also gains predictive power over future elements of the sequence. </p>
<p>This analogy between compression and understanding cuts both ways. For example by recognizing the depicted objects on a painting, I am at the same time also compressing my internal representation of the painting.</p>
<p>[^1]: The upper bound on the expressive strength, assuming that brains are essentially Turing machines, is storing a representation in some sentence of a Turing complete language.</p>
<p>[^2]: Of course the agent will associate some form of probability that this representation is correct.</p>
</div><a class="read-more" href="/2017/08/15/compression/">&hellip; more</a></article></li></ul></main><footer><div class="paginator"></div><div class="copyright"><p>&copy; 2016 - 2017 <a href="http://mathijshenquet.github.io">Mathijs Henquet</a>.<br>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/Dreyer/hexo-theme-artemis" target="_blank">Artemis</a>.</p></div></footer></div><script>var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-12345678-9']);
_gaq.push(['_trackPageview']);

(function () {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();</script></body></html>